# -*- coding: utf-8 -*-
"""Image_Cap.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ll31vpdW0EV6t-uaIPxO5iG-DoxWcFRt
"""

import os
import numpy as np
import h5py
import json
import torch
from PIL import Image
!pip install tqdm
from tqdm import tqdm
from collections import Counter
from random import seed, choice, sample
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import zipfile
import torch.nn as nn
import torchvision
import torch.nn.init as init
from torch.nn.utils.rnn import pack_padded_sequence
import matplotlib.pyplot as plt

def create_input_files(dataset, karpathy_json_path, image_folder, captions_per_image, min_word_freq, output_folder,
                       max_len=50):
    """
    Creates input files for training, validation, and test data.
    """
    return
    # Read Karpathy JSON
    with open(karpathy_json_path, 'r') as j:
        data = json.load(j)

    # Read image paths and captions for each image
    train_image_paths = []
    train_image_captions = []
    test_image_paths = []
    test_image_captions = []
    word_freq = Counter()

    for img in data['images']:
        captions = []
        for c in img['sentences']:
            word_freq.update(c['tokens'])
            if len(c['tokens']) <= max_len:
                captions.append(c['tokens'])

        if len(captions) == 0:
            continue

        path = os.path.join(image_folder, img['filename'])

        if img['split'] in {'train'}:
            train_image_paths.append(path)
            train_image_captions.append(captions)
        elif img['split'] in {'test'}:
            test_image_paths.append(path)
            test_image_captions.append(captions)

    assert len(train_image_paths) == len(train_image_captions)
    assert len(test_image_paths) == len(test_image_captions)

    words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]
    word_map = {k: v + 1 for v, k in enumerate(words)}
    word_map['<unk>'] = len(word_map) + 1
    word_map['<start>'] = len(word_map) + 1
    word_map['<end>'] = len(word_map) + 1
    word_map['<pad>'] = 0

    base_filename = dataset + '_' + str(captions_per_image) + '_cap_per_img_' + str(min_word_freq) + '_min_word_freq'

    with open(os.path.join(output_folder, 'WORDMAP_' + base_filename + '.json'), 'w') as j:
        json.dump(word_map, j)

    seed(123)
    for impaths, imcaps, split in [(train_image_paths, train_image_captions, 'TRAIN'),
                                   (test_image_paths, test_image_captions, 'TEST')]:

        h5_file_path = os.path.join(output_folder, split + '_IMAGES_' + base_filename + '.hdf5')

        with h5py.File(h5_file_path, 'a') as h:
            if 'images2' in h:
                del h['images2']

            h.attrs['captions_per_image'] = captions_per_image
            images = h.create_dataset('images2', (len(impaths), 3, 256, 256), dtype='uint8')

            print("\nReading %s images and captions, storing to file...\n" % split)

            enc_captions = []
            caplens = []

            for i, path in enumerate(tqdm(impaths)):

                if len(imcaps[i]) < captions_per_image:
                    captions = imcaps[i] + [choice(imcaps[i]) for _ in range(captions_per_image - len(imcaps[i]))]
                else:
                    captions = sample(imcaps[i], k=captions_per_image)

                assert len(captions) == captions_per_image

                img = Image.open(path).convert('RGB')
                img = img.resize((256, 256), Image.LANCZOS)
                img = np.array(img)
                img = img.transpose(2, 0, 1)
                assert img.shape == (3, 256, 256)
                assert np.max(img) <= 255

                images[i] = img

                for j, c in enumerate(captions):
                    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in c] + [word_map['<end>']] + [word_map['<pad>']] * (max_len - len(c))
                    c_len = len(c) + 2
                    enc_captions.append(enc_c)
                    caplens.append(c_len)

            # Debugging information
            print(f"Number of images: {images.shape[0]}")
            print(f"Expected number of captions: {images.shape[0] * captions_per_image}")
            print(f"Actual number of encoded captions: {len(enc_captions)}")
            print(f"Actual number of caption lengths: {len(caplens)}")

            assert images.shape[0] * captions_per_image == len(enc_captions) == len(caplens)

            with open(os.path.join(output_folder, split + '_CAPTIONS_' + base_filename + '.json'), 'w') as j:
                json.dump(enc_captions, j)

            with open(os.path.join(output_folder, split + '_CAPLENS_' + base_filename + '.json'), 'w') as j:
                json.dump(caplens, j)

if __name__ == '__main__':
    # Create input files (along with word map)
    create_input_files(dataset='flickr8k',
                       karpathy_json_path='Image cap/flickr8k/dataset_flickr8k.json',
                       image_folder='Image cap/flickr8k/Images/Images',
                       captions_per_image=5,
                       min_word_freq=5,
                       output_folder='Image cap/flickr8k/output',
                       max_len=50)

def save_checkpoint(epoch,encoder,decoder,decoder_optimizer):
  state={
      'epoch':epoch,
      'encoder':encoder,
      'decoder':decoder,
      'decoder_optimizer':decoder_optimizer}

  filename='checkpoint' + str(epoch) +'.pth'
  torch.save(state,filename)


class AverageMeter(object):
    """
    Keeps track of most recent, average, sum, and count of a metric.
    """

    def __init__(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

def adjust_learning_rate(optimizer, shrink_factor):
    """
    Shrinks learning rate by a specified factor.

    :param optimizer: optimizer whose learning rate must be shrunk.
    :param shrink_factor: factor in interval (0, 1) to multiply learning rate with.
    """

    print("\nDECAYING learning rate.")
    for param_group in optimizer.param_groups:
        param_group['lr'] = param_group['lr'] * shrink_factor
    print("The new learning rate is %f\n" % (optimizer.param_groups[0]['lr'],))

def accuracy(scores, targets, k):
    """
    Computes top-k accuracy, from predicted and true labels.

    :param scores: scores from the model--->(batch_size,max(decoding_len),vocab_size)-----> (sum(decoding_length),vocab_size)
    :param targets: true labels----->(batch_size,max(decoding_len))--->(sum(decoding_length))
    :param k: k in top-k accuracy
    :return: top-k accuracy
    """

    batch_size = targets.size(0)
    _, ind = scores.topk(k, 1, True, True) # taking top k preds---> values,indices
    correct = ind.eq(targets.view(-1, 1).expand_as(ind)) # checking if they are equal and expanding the dim of target to match with scores
    correct_total = correct.view(-1).float().sum()  # 0D tensor
    return correct_total.item() * (100.0 / batch_size)

class Flickr8Dataset(Dataset):
  def __init__(self,data_transforms):
    self.h=h5py.File('drive/MyDrive/TRAIN_IMAGES_flickr8k_5_cap_per_img_5_min_word_freq.hdf5','r')
    self.imgs=self.h['images2']
    self.cpi=self.h.attrs['captions_per_image']

    with open('TRAIN_CAPTIONS_flickr8k_5_cap_per_img_5_min_word_freq.json','r') as j:
      self.captions=json.load(j)

    with open('TRAIN_CAPLENS_flickr8k_5_cap_per_img_5_min_word_freq.json','r') as j:
      self.caplens=json.load(j)

    self.transforms=data_transforms
    self.dataset_size=len(self.captions)

  def __len__(self):
    return self.dataset_size

  def __getitem__(self,i):  #run for single input or you can say that if batch size is 10 then it will run 10 times

    img = torch.FloatTensor(self.imgs[i//self.cpi]/255.)
    #img = torch.FloatTensor(img)  # Convert to float tensor
    if self.transforms is not None:
        #img = torch.FloatTensor(img)
        img=self.transforms(img)

    caption=torch.LongTensor(self.captions[i])
    caplen=torch.LongTensor([self.caplens[i]])

    return img,caption,caplen

class Encoder(nn.Module):
    def __init__(self):
        super(Encoder,self).__init__()
        resnet=torchvision.models.resnet101(pretrained=True)
        all_modules=list(resnet.children())
        modules=all_modules[:-2]
        self.resnet= nn.Sequential(*modules)
        self.avgpool=nn.AvgPool2d(8)
        self.fine_tune()

    def fine_tune(self):
        for p in self.resnet.parameters():
            p.requires_grad = False

    def forward(self,images):
        batch_size= images.shape[0]
        encoded_image=self.resnet(images)
        global_features=self.avgpool(encoded_image).view(batch_size,-1)
        return global_features

class Decoder(nn.Module):
    def __init__(self,embed_dim,decoder_dim,vocab_size,encoder_dim=2048):
        super(Decoder,self).__init__()
        self.embed_dim=embed_dim
        self.decoder_dim=decoder_dim
        self.vocab_size=vocab_size
        self.encoder_dim=encoder_dim
        self.embedding=nn.Embedding(vocab_size,embed_dim)
        self.lstm=nn.LSTMCell(embed_dim+encoder_dim,decoder_dim)
        self.fc=nn.Linear(decoder_dim,vocab_size)
        self.init_weights()

    def init_weights(self):
        init.uniform_(self.embedding.weight, -0.1, 0.1)
        init.uniform_(self.fc.weight, -0.1, 0.1)
        init.constant_(self.fc.bias, 0)
    def init_hidden_states(self,batch_size):
        h=torch.zeros(batch_size,self.decoder_dim).to(device)
        c=torch.zeros(batch_size,self.decoder_dim).to(device)
        return h,c

    def forward(self,global_image,encoded_captions,caption_lengths):
        batch_size=global_image.size(0)
        encoder_dim=global_image.size(-1)
        caption_lengths,sort_ind=caption_lengths.squeeze(1).sort(dim=0,descending=True)
        global_image = global_image[sort_ind]
        encoded_captions = encoded_captions[sort_ind]
        embeddings=self.embedding(encoded_captions)
        h,c=self.init_hidden_states(batch_size)
        decode_lengths= (caption_lengths -1).tolist()
        predictions=torch.zeros(batch_size,max(decode_lengths),self.vocab_size).to(device)

        for t in range(max(decode_lengths)):
            batch_size_t= sum([l>t for l in decode_lengths])
            lstm_input= torch.cat([embeddings[:batch_size_t,t,:],global_image[:batch_size_t]],dim=-1)
            h,c=self.lstm(lstm_input,(h[:batch_size_t],c[:batch_size_t]))
            preds=self.fc(h)
            predictions[:batch_size_t,t,:]=preds

        return predictions,encoded_captions,decode_lengths,sort_ind

def train(train_loader, encoder, decoder, criterion, decoder_optimizer, epoch):
    """
    Performs one epoch's training.

    :param train_loader: DataLoader for training data
    :param encoder: encoder model
    :param decoder: decoder model
    :param criterion: loss layer
    :param encoder_optimizer: optimizer to update encoder's weights (if fine-tuning)
    :param decoder_optimizer: optimizer to update decoder's weights
    :param epoch: epoch number
    """

    decoder.train()  # train mode (dropout and batchnorm is used)
    encoder.train()

    #batch_time = AverageMeter()  # forward prop. + back prop. time
   # data_time = AverageMeter()  # data loading time
    losses = AverageMeter()  # loss (per word decoded)
    top3accs = AverageMeter()  # top5 accuracy

   # start = time.time()

    # Batches
    for i, (img, caption, caplen) in enumerate(train_loader):
        #data_time.update(time.time() - start)
        img=img.to(device)
        caption=caption.to(device)
        caplen=caplen.to(device)

        # Forward prop.
        global_features = encoder(img)
        scores, caps_sorted, decode_lengths,sort_ind = decoder(global_features, caption, caplen)

        # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>
        targets = caps_sorted[:, 1:]

        # Remove timesteps that we didn't decode at, or are pads
        # pack_padded_sequence is an easy trick to do this
        scores= pack_padded_sequence(scores, decode_lengths, batch_first=True)
        targets= pack_padded_sequence(targets, decode_lengths, batch_first=True)

        # Calculate loss
        loss = criterion(scores.data, targets.data)

        # Add doubly stochastic attention regularization
        decoder_optimizer.zero_grad()
        loss.backward()
        decoder_optimizer.step()
        top3= accuracy(scores.data,targets.data,3)
        losses.update(loss.item(),sum(decode_lengths))
        top3accs.update(top3,sum(decode_lengths))

        if i % print_freq == 0:
            print('Epoch: [{0}][{1}/{2}]\t'
                  'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
                  'Top-3 Accuracy {top3.val:.3f} ({top3.avg:.3f})'.format(epoch, i, len(train_loader),

                                                                         loss=losses,
                                                                          top3=top3accs))

device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
embed_dim=512
decoder_dim=512
decoder_lr=5e-4
start_epoch=0
epochs=10
batch_size=60
print_freq=100
checkpoint='checkpoint8.pth'

with open('WORDMAP_flickr8k_5_cap_per_img_5_min_word_freq.json','r') as j:
    word_map=json.load(j)
rev_word_map={v:k for k,v in word_map.items()}

if checkpoint is None:
    encoder=Encoder()
    decoder=Decoder(embed_dim,decoder_dim=decoder_dim,vocab_size=len(word_map))
    decoder_optimizer=torch.optim.Adam(decoder.parameters(),lr=decoder_lr)
else:
    checkpoint=torch.load(checkpoint)
    start_epoch=checkpoint['epoch'] +1
    decoder= checkpoint['decoder']
    decoder_optimizer= checkpoint['decoder_optimizer']
    encoder=checkpoint['encoder']
encoder=encoder.to(device)
decoder=decoder.to(device)


criterion= nn.CrossEntropyLoss().to(device)
normalize=transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])
transform=transforms.Compose([normalize])

train_loader=torch.utils.data.DataLoader(Flickr8Dataset(transform),batch_size=batch_size,shuffle=True,pin_memory=True)

def greedy_decode(image):
    """
    Perform greedy decoding to generate a caption for a given image.

    :param image: Path to the image file
    :param encoder: Encoder model
    :param decoder: Decoder model
    :param word_map: Dictionary mapping words to indices
    :param rev_word_map: Dictionary mapping indices to words
    :param device: Device to perform computations on (CPU or GPU)
    """

    encoder.eval()
    decoder.eval()

    max_len = 20  # Maximum length of generated caption

    # Open and display the image
    img = Image.open(image)
    plt.imshow(img)
    plt.show()

    # Convert image to numpy array and normalize
    img = np.array(img)
    img = Image.fromarray(img).resize((256, 256))
    img = np.array(img) / 255.0

    # Convert to PyTorch tensor and apply normalization
    img = torch.FloatTensor(img.transpose(2, 0, 1)).to(device)
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    transform = transforms.Compose([normalize])
    img = transform(img).unsqueeze(0)  # Add batch dimension and move to device

    # Extract global features from the image
    global_features = encoder(img)

    # Initialize decoder input and hidden states
    pred = torch.LongTensor([[word_map['<start>']]])
    h, c = decoder.init_hidden_states(1)

    sampled = []

    for timestep in range(max_len):
        embeddings = decoder.embedding(pred).squeeze(1)
        lstm_input = torch.cat([embeddings, global_features], dim=-1)
        h, c = decoder.lstm(lstm_input, (h, c))
        preds = decoder.fc(h)
        _, pred = preds.max(1)
        sampled.append(pred.item())

        if pred.item() == word_map['<end>']:
            break

    # Convert indices to words
    generated_words = [rev_word_map[idx] for idx in sampled]
    filtered_words = " ".join(generated_words)
    print(filtered_words)

for epoch in range(start_epoch, epochs):

    # Adjust learning rate every 3 epochs, except at epoch 0
    if epoch % 3 == 0 and epoch != 0:
        adjust_learning_rate(decoder_optimizer, 0.8)

    # Perform training for the current epoch
    train(train_loader=train_loader,
          encoder=encoder,
          decoder=decoder,
          criterion=criterion,
          decoder_optimizer=decoder_optimizer,
          epoch=epoch)

    # Save the model checkpoint
    save_checkpoint(epoch, encoder, decoder, decoder_optimizer)











